{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from my_jax_utils import show_hlo_info\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x = jnp.zeros((1024*128,2)) # Our probe array -- it uses 1MB\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Concepts:\n",
    "* **GPU kernel** A GPU program that is executed parallel. Kernels cannot communicate directly\n",
    "* **Device Arrays** An array that lives in GPU RAM -- usually this is the data that needs to be Input, Output or transfered between GPU kernels (but not the kernel-internal data.)\n",
    "* **Graph** An arangement of several GPU kernels (+ their internal operations). The most important aspects are the boundaries between kernels\n",
    "* **temporaries** Device arrays that need to be temporarily created and passed between kernels, but that may be discarded later\n",
    "* **Fusing** Grouping of several operations as internal operations inside of a kernel / Merging of kernels to reduce the number of kernels launches\n",
    "* **Fusion Barriers** Some operations prevent that code before and after them can be fused into one kernel, e.g. **FFTs**, **reduction operations** (jnp.sum), ... "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# An example Graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kvec_mesh(N):\n",
    "    return jnp.stack((jnp.arange(0, N), jnp.arange(0, N)), axis=-1)\n",
    "\n",
    "def f_sum_jax(x):\n",
    "    kvm = kvec_mesh(len(x))\n",
    "    return x * 2 + jnp.sum(kvm * x, axis=1, keepdims=True)\n",
    "\n",
    "show_hlo_info(jax.jit(f_sum_jax), x, width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "Above you can see an example of a Graph. The grey boxes indicate the boundaries of different kernels (Watch out to not confuse with \"sub-computations\" which are groups of kernels that appear in white boxes -- e.g. when using a loop). Inside of each kernel you can see all the operations that jax managed to \"fuse\" together. \n",
    "\n",
    "As a good guidline for optimizing both performance and memory usage assume:\n",
    "\n",
    "* Computations inside of a kernel are basically for free (they need no memory and have negligible performance cost)\n",
    "\n",
    "Therefore, you should be mainly conernced with\n",
    "\n",
    "* How many kernels are used (because of memory bandwith)\n",
    "* How many arrows appear between the kernels (each corresponding to a (temporary) device array -- increasing memory usage)\n",
    "\n",
    "The graph above is sub-optimal, because we could have fused everything into one kernel if we wrote the program slightly differently (I will show this later later.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "# Important general considerations:\n",
    "\n",
    "* Minimize temporary memory usage\n",
    "* Minimize kernel launches / Maximize fusion\n",
    "* Recomputing intermediates inside of a kernel is very cheap and almost always better than keeping temporaries in device memory.\n",
    "\n",
    "Explanation: Most commonly GPU kernels are limited by the memory bandwith. The more data has to go through I/O, the worse the performance. If recalculation takes ~ 100 floating point operations, it is the better option performance wise (and of course saves memory). Optimizing memory therefore in most circumstances also optimizes performance.\n",
    "\n",
    "* Some operations can create **fusion barriers** (e.g. `jnp.sum` or `jnp.fft.rfftn`)\n",
    "* Using **jax.jit** in a sub-function does not create a fusion barrier (I.e. it is safe, but might waste tiny amount of compilation time)\n",
    "* For cheap-to-calculate \"constants\" that you use many times (e.g. a kmesh), prefer to put them into functions that you call for each usage. This is especially important for achieveing recalculation inside of loops\n",
    "* Sometimes we might need to trick jack into doing the recomputation rather than saving intermediates as temporaries\n",
    "* Avoid constant folding at all costs!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "# Important details\n",
    "Throughout the notebook I will conduct many experiments for creating graphs to see what XLA can and can't do. Here I summarize the most important guidlines that I have extracted from those experiments. To appreciate these scroll down and try to understand the corresponding experiments\n",
    "\n",
    "\n",
    "* **Avoid reduction operators** (if possible), they will generally not be fused. For example\n",
    "```python\n",
    "dot_prod = jnp.sum(x1*x2, axis=-1)                 # Bad for jax, prevents fusion in some cases\n",
    "dot_prod = x1[...,0]*x2[...,0] + x1[...,1]*x2[...,1] + x1[...,2]*x2[...,2] # always allows fusion \n",
    "```\n",
    "* Kernels with **aligned input and output** work effectively in place, not needing an additional temporary array. Therefore, they never have any impact on memory usage!\n",
    "* Kernels where **input and output misalign** -- e.g. transposition kernels -- cannot work in place and generally need an additional temporary array.\n",
    "* Often the same holds for external library kernels (e.g. ffts or sort)\n",
    "* For **FFTs** with vectorial data prefer (3,N,N,N) layout over (N,N,N,3) layout, since (3,N,N,N) requires no transpositions on jax's side (internally they do, but whatever library jax calls does a better job with the memory usage)\n",
    "* Jax loops can be partially **unrolled** to allow fusion. This may be worth it when the kernel that is looped over is very cheap (so that launch and I/O overhead dominate)\n",
    "* XLA does **Common Sub Expression elimination** (CSE) that finds identical expressions. This is very useful in many scenarios, but it creates a nightmare for writing stable and memory-optimal code:\n",
    "    - If the same expression appears more than once inside the same scope, jax identifies it and treats it as if it was the same tracer\n",
    "    - If the computation is sufficiently complex, xla prefers to store it in a temporary device array from where it is first to where it is last needed (e.g. random numbers tend to be in this category)\n",
    "    - If it is a reasonably simple expression, xla prefers to recalculate it locally inside of each kernel (no memory required) (e.g. `jnp.arange` or a kmesh Fourier Grid)\n",
    "    - The boundary between the two cases is somewhat **unpredictable** and may be crossed because of tangible changes (e.g. in one example using  `jnp.sin` instead of `jnp.exp`).\n",
    "    - The CSE seems to only apply inside of the same \"**scope**\". E.g. if you declare a tracer outside of a loop and use it in the loop, it will never be recalculated in the loop always creating a temporary (avoid this!)\n",
    "    - If you redefine the computation inside of the loop and outside of the loop, it will always be recalculated inside of the loop. (So recreating a tracer, e.g. through a function, will have a different effect than reusing an identical tracer.)\n",
    "    - As of know, jax does not offer a way to deactivate CSE selectively. So beyond the scoping constraints the only way to guarantee recomputation may be to try to trick jax. (Note: `jax.lax.optimization_barrier` does too much, it also deactivates fusion, which is generally unacceptable. `jax.remat(... prevent_cse=True)` seems to only apply to a gradient pass and not do anything in a normal computation). I created a feature suggestion [here](https://github.com/jax-ml/jax/issues/32544)... let's see what will happen\n",
    "* Since recalculation is almost always better than having a temporary array (especially if it can be fused into the kernel), the following approach will likely be as-good-as-it-gets in most cases:\n",
    "    - For every \"**constant**\" that can be cheaply recalculated **define a (pure jax) function**. E.g. get_kmesh(N) or so\n",
    "    - Wherever you need the constant, call the function\n",
    "    - In 90% of cases this should lead to consistent recalculation and be optimal\n",
    "    - For the other 10% it may be worth it to inspect the graph and see wheter it actually gets recalculated in each kernel\n",
    "    - If XLA decides to create a temporary, (happens if the calculation is sufficiently expensive and two calls to the function exist in the same scope, but cannot be fused) for now, there is only one hacky way out:\n",
    "    - Each time you reuse the variable modify its calculation by a different epsilon so that the expressions cannot be identified.\n",
    "* Sometimes creating a meshgrid with the help of `jnp.indices` can be better than using `jnp.meshgrid` + reductions. This way we can often ensure that the kernel is launched in the right layout and avoid a fusion barrier.\n",
    "* Please find an optimal implementation of getting a k2 grid that can easily be fused later in this notebook\n",
    "* Folded constants (e.g. trace-time-known numpy arrays) contribute to device memory usage. They appear under `generated_code_size_in_bytes` in `fcompiled.memory_analysis()`. **Avoid large folded constants** (>1MB) at all costs!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "# Fusing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_well_fused(x):\n",
    "    \"\"\"In general jax does a really good job at fusing (and not creating temporaries)\"\"\"\n",
    "    a = x * 2\n",
    "    b = jnp.arange(a.shape[0])[:,None] * 3\n",
    "    c = x[0] + x - a\n",
    "    return jnp.sin(a) + a**2 + b + c\n",
    "\n",
    "# Note that all operations are fused into one kernel (i.e. the grey box)\n",
    "# This program is likely a very-close-to optimal GPU program for this computation\n",
    "# Generally our goal should be to write code in such a way that jax can fuse as much as possible\n",
    "show_hlo_info(jax.jit(f_well_fused), x, width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "# Fuse barriers\n",
    "Some operations cannot fuse, creating barriers (e.g. jnp.sum can do that)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kvec_mesh(N):\n",
    "    return jnp.stack((jnp.arange(0, N), jnp.arange(0, N)), axis=-1)\n",
    "\n",
    "def f_sum_jax(x):\n",
    "    \"\"\"Shows: Jax generally does not fuse jnp.sum so don't use it if you can avoid it!\n",
    "    compare against: f_sum_explicit \n",
    "    \"\"\"\n",
    "    kvm = kvec_mesh(len(x))\n",
    "    return x * 2 + jnp.sum(kvm * x, axis=1, keepdims=True)\n",
    "\n",
    "def f_sum_explicit(x):\n",
    "    kvm = kvec_mesh(len(x))\n",
    "    return x * 2 + (kvm[...,0] * x[...,0] + kvm[...,1] * x[...,1])[:,None]\n",
    "\n",
    "show_hlo_info(jax.jit(f_sum_jax), x)\n",
    "show_hlo_info(jax.jit(f_sum_explicit), x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## jitting sub functions\n",
    "Jitting sub-functions does **not** prevent fusion!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finternal(x):\n",
    "    return x * 2 + 25.0\n",
    "finternal.jit = jax.jit(finternal, inline=False)\n",
    "\n",
    "def f_with_subjit(x):\n",
    "    \"\"\"This fuses fine, allthough we jitted our sub-function\"\"\"\n",
    "    return finternal.jit(x) + 3\n",
    "\n",
    "show_hlo_info(jax.jit(f_with_subjit), x, width=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "# Intentional Fusion barrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_with_subjit(x):\n",
    "    \"\"\"Becomes two kernels, because we have a barrier inbetween\"\"\"\n",
    "    S = jax.lax.optimization_barrier(jnp.sin(x) + 3)\n",
    "    return S + jnp.exp(S)\n",
    "\n",
    "show_hlo_info(jax.jit(f_with_subjit), x, width=300) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "# FFTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_fft3d(x):\n",
    "    \"\"\"I was surprised that we need 2x temporaries (rarther than 1x) here!\n",
    "    This is, because the FFT dimension always has to be the last one, and jax needs to hold the input\n",
    "    and the output of the Fourier space transpose in memory at the same time.\n",
    "\n",
    "    compare against: f_fft3d_transposed\n",
    "    \"\"\"\n",
    "    a = jnp.fft.rfftn(x, axes=(0,1,2))\n",
    "    b = jnp.fft.irfftn(a, axes=(0,1,2))\n",
    "    return b\n",
    "\n",
    "def f_fft3d_transposed(x):\n",
    "    \"\"\"We can reduce the memory usage by having Input and Output transposed as (2, N,N,N) \n",
    "    # instead of (N,N,N,2).\n",
    "    Note: It seems kernels which are working in place can easily use Input = Output and do\n",
    "    not need two temporaries, even if Input and Output are temporary. The transposition kernel\n",
    "    cannot do this however! (I understand why, but I think it is not-optimal in XLA)\n",
    "    \"\"\"\n",
    "    a = jnp.fft.rfftn(x, axes=(1,2,3))\n",
    "    b = jnp.fft.irfftn(a*2, axes=(1,2,3))\n",
    "    return b + 3\n",
    "\n",
    "x3d = jnp.zeros((64,64,32,2)) # Our probe array -- it uses 1MB\n",
    "show_hlo_info(jax.jit(f_fft3d), x3d, width=250)\n",
    "show_hlo_info(jax.jit(f_fft3d_transposed), x3d.transpose(3,0,1,2), width=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "# Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_for(x, unroll=None):\n",
    "    \"\"\"Loops save their state variables as temporaries -- because each iteration launches a kernel\n",
    "    If you have a very short loop, consider fully unrolling it.\n",
    "    \"\"\"\n",
    "    def add_x(i, x):\n",
    "        return x*x + 1\n",
    "    y = jax.lax.fori_loop(0, 4, add_x, x, unroll=unroll)\n",
    "    return y\n",
    "\n",
    "show_hlo_info(jax.jit(f_for, static_argnames=\"unroll\"), x, width=800)\n",
    "# Partially unrolled, saves half of the kernel launches and I/O, but still needs a temporary\n",
    "show_hlo_info(jax.jit(f_for, static_argnames=\"unroll\"), x, unroll=2, width=800)\n",
    "# Fully unrolled, becomes a single kernel avoids 1 temporary:\n",
    "# (Not sure whether this is any different than a pure python loop)\n",
    "show_hlo_info(jax.jit(f_for, static_argnames=\"unroll\"), x, unroll=4, width=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "# Intermediates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_recalc(x):\n",
    "    \"\"\"Here jax smartly recalculateds an intermediate rather than storing it.\n",
    "\n",
    "    Jax sometimes recalculates intermediates and sometimes stores them. It is very unpredictable.\n",
    "    \"\"\"\n",
    "    im = jnp.exp(jnp.arange(x.size).reshape(x.shape) ** 2.3)\n",
    "\n",
    "    # Here we use a sum to create a fusion barrier\n",
    "    # Imagine this represents a long calculation that cannot be fused\n",
    "    a = x + jnp.sum(im, axis=-1, keepdims=True) \n",
    "\n",
    "    # Later we use \"im\" again. Jax recalculates it rather than storing it, great!\n",
    "    return a + im \n",
    "\n",
    "def f_stored(x):\n",
    "    \"\"\"(Note the two outputs passed and the recalculation part of the second graph missing)\"\"\"\n",
    "    im = jnp.sin(jnp.arange(x.size).reshape(x.shape) ** 2.3) # Tiny difference, but changes graph a lot!\n",
    "    a = x + jnp.sum(im, axis=-1, keepdims=True) \n",
    "\n",
    "    return a + im\n",
    "\n",
    "show_hlo_info(jax.jit(f_recalc), x, width=700)\n",
    "show_hlo_info(jax.jit(f_stored), x, width=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "# Common sub-expression elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_im(x, eps=0.):\n",
    "    return jnp.sin((jnp.arange(x.size).reshape(x.shape) + eps) ** 2.3)\n",
    "\n",
    "def f_cse(x):\n",
    "    \"\"\"Here jax finds the common sub-expression and calculates it once\"\"\"\n",
    "    a = x + jnp.sum(get_im(x), axis=-1, keepdims=True) \n",
    "\n",
    "    b =  jnp.sum(a + get_im(x))\n",
    "\n",
    "    return a + get_im(x) + b\n",
    "\n",
    "def f_cse_loop_expl(x):\n",
    "    \"\"\"Here it does, too, because we told it explicitly\"\"\"\n",
    "    im = get_im(x)\n",
    "\n",
    "    def step(i, a):\n",
    "        return a + im\n",
    "\n",
    "    return jax.lax.fori_loop(0, 5, step, x + im)\n",
    "\n",
    "def f_cse_loop_impl(x):\n",
    "    \"\"\"Here it does not find the common sub-expression, it will be recalculated\n",
    "    Likely loops live in a different scope and CSE does not identify sub-expressions across scopes\n",
    "    (this is actually what we want! Uses less memory)\"\"\"\n",
    "    def step(i, a):\n",
    "        return a + get_im(x)\n",
    "\n",
    "    return jax.lax.fori_loop(0, 5, step, x + get_im(x))\n",
    "\n",
    "def f_cse_loop_after(x):\n",
    "    \"\"\"Finds a common sub-expression between pre-and post-loop... increase memory again...\"\"\"\n",
    "    def step(i, a):\n",
    "        return a + get_im(x)\n",
    "\n",
    "    return jax.lax.fori_loop(0, 5, step, x + get_im(x)) + get_im(x) # if we put eps = 1e-10 here, it will not find it\n",
    "\n",
    "show_hlo_info(jax.jit(f_cse), x, width=800)\n",
    "show_hlo_info(jax.jit(f_cse_loop_expl), x, width=1000)\n",
    "show_hlo_info(jax.jit(f_cse_loop_impl), x, width=1000)\n",
    "show_hlo_info(jax.jit(f_cse_loop_after), x, width=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "# How to optimally recalculate a Fourier k-mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kmesh_naive(rho, d=1., norm2=False, real=True):\n",
    "    \"\"\"In general d=boxsize/(2*np.pi*rho.shape[0])\n",
    "    Here with norm2=True, we tend to get two kernels\"\"\"\n",
    "    kx = jnp.fft.fftfreq(rho.shape[0], d=d)\n",
    "    ky = jnp.fft.fftfreq(rho.shape[1], d=d)\n",
    "    if real:\n",
    "        kz = jnp.fft.rfftfreq(rho.shape[2], d=d)\n",
    "    else:\n",
    "        kz = jnp.fft.fftfreq(rho.shape[2], d=d)\n",
    "\n",
    "    kx,ky,kz = jnp.meshgrid(kx, ky, kz, indexing=\"ij\")\n",
    "    if norm2:\n",
    "        return kx**2 + ky**2 + kz**2\n",
    "    else:\n",
    "        return jnp.stack((kx,ky,kz), axis=-1)\n",
    "\n",
    "def myfftfreq(i, n, d=1.0):\n",
    "    \"\"\"Does the same as jnp.fft.fftfreq(n, d)[i] but avoids creating the frequency array\"\"\"\n",
    "    return jnp.where(i < (n+1)//2, i / n, (i - n)/n) / d\n",
    "def myrfftfreq(i, n, d=1.0):\n",
    "    \"\"\"Does the same as jnp.fft.rfftfreq(n, d)[i] but avoids creating the frequency array\"\"\"\n",
    "    return i / n / d\n",
    "\n",
    "for n in [3, 8, 16, 17, 32, 64, 128]: # Test that our fftfreq implementation is correct\n",
    "    assert(jnp.all(jnp.fft.fftfreq(n) == myfftfreq(jnp.arange(n), n)))\n",
    "    assert(jnp.all(jnp.fft.rfftfreq(n) == myrfftfreq(jnp.arange((n//2)+1), n)))\n",
    "\n",
    "def get_kmesh(rho, d=1., norm2=False, real=True):\n",
    "    \"\"\"By starting with `jnp.indices` we are more likely to gurantee to achieve a single kernel,\n",
    "    that can easily be fused later on, since the kernel dispatch dimensions will likely be \n",
    "    identical with the needed output dimensions\n",
    "    \"\"\"\n",
    "    ix,iy,iz = jnp.indices((rho.shape[0], rho.shape[1], (rho.shape[2]//2)+1 if real else rho.shape[2]))\n",
    "    \n",
    "    kx = myfftfreq(ix, rho.shape[0], d=d)\n",
    "    ky = myfftfreq(iy, rho.shape[1], d=d)\n",
    "\n",
    "    if real:\n",
    "        kz = myrfftfreq(iz, rho.shape[2], d=d)\n",
    "    else:\n",
    "        kz = myfftfreq(iz, rho.shape[2], d=d)\n",
    "    \n",
    "    if norm2:\n",
    "        return kx**2 + ky**2 + kz**2\n",
    "    else:\n",
    "        return jnp.stack((kx,ky,kz), axis=-1)\n",
    "\n",
    "show_hlo_info(jax.jit(get_kmesh_naive, static_argnames=(\"norm2\", \"real\")), jnp.zeros((64,64,64)), norm2=True, width=600)\n",
    "show_hlo_info(jax.jit(get_kmesh, static_argnames=(\"norm2\", \"real\")), jnp.zeros((64,64,64)), norm2=True, width=800)\n",
    "\n",
    "for n in [3, 8, 16, 17, 32, 64, 128]: \n",
    "    xin = jnp.zeros((n,n,n))\n",
    "    km1 = get_kmesh_naive(xin, norm2=True)\n",
    "    km2 = get_kmesh(xin, norm2=True)\n",
    "    assert(jnp.allclose(km1, km2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_filter_with_temp(rho):\n",
    "    \"\"\"If we define a variable outside the loop, it will be stored as a temporary\n",
    "    \n",
    "    This is confusing, because usually XLA would determine that k2 is worth recalculating.\n",
    "    It seems likely that XLA assumes that all variables that are defined outside of the loop\n",
    "    should not be treated as a graph, but as a constant\n",
    "    \"\"\"\n",
    "    k2 = get_kmesh(rho, norm2=True)\n",
    "\n",
    "    def step(i, rho):\n",
    "        rhok = jnp.fft.rfftn(rho, axes=(0,1,2))\n",
    "        rhok *= jnp.exp(-0.5 * k2)\n",
    "        return jnp.fft.irfftn(rhok, axes=(0,1,2))\n",
    "\n",
    "    return jax.lax.fori_loop(0, 5, step, rho)\n",
    "\n",
    "def f_filter_with_recalc(rho):\n",
    "    \"\"\"Triggers recalculation and everything gets fused into one kernel -- optimal!\"\"\"\n",
    "    def step(i, rho):\n",
    "        rhok = jnp.fft.rfftn(rho, axes=(0,1,2))\n",
    "        rhok *= jnp.exp(-0.5 * get_kmesh(rho, norm2=True))\n",
    "        return jnp.fft.irfftn(rhok, axes=(0,1,2))\n",
    "\n",
    "    return jax.lax.fori_loop(0, 5, step, rho)\n",
    "\n",
    "def f_filter_with_recalc_naive(rho):\n",
    "    \"\"\"This triggers recalculation -- but requires 2 kernel launches in the loop\n",
    "    surprisingly the memory usage is not worse than in the better implementation above\n",
    "    \"\"\"\n",
    "    def step(i, rho):\n",
    "        rhok = jnp.fft.rfftn(rho, axes=(0,1,2))\n",
    "        rhok *= jnp.exp(-0.5 * get_kmesh_naive(rho, norm2=True))\n",
    "        return jnp.fft.irfftn(rhok, axes=(0,1,2))\n",
    "\n",
    "    return jax.lax.fori_loop(0, 5, step, rho)\n",
    "\n",
    "show_hlo_info(jax.jit(f_filter_with_temp), jnp.zeros((64,64,64)), width=800)\n",
    "show_hlo_info(jax.jit(f_filter_with_recalc), jnp.zeros((64,64,64)), width=800)\n",
    "show_hlo_info(jax.jit(f_filter_with_recalc_naive), jnp.zeros((64,64,64)), width=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "# Constant folding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def f_numpy(x):\n",
    "    \"\"\"Folded constants appear as 'code_size' in the memory report. They live in device memory \n",
    "    space and they contribute to peak memory! Also they absolutely obliterate the compilation time!\n",
    "    \"\"\"\n",
    "    return x + np.arange(x.size).reshape(x.shape) ** 2.3\n",
    "\n",
    "def f(x):\n",
    "    return x + jnp.arange(x.size).reshape(x.shape) ** 2.3\n",
    "\n",
    "x2 = jnp.zeros((1024*1024*16,2)) # Our probe array -- it uses 1MB\n",
    "show_hlo_info(jax.jit(f_numpy), x2) # np array gets always folded\n",
    "show_hlo_info(jax.jit(f), x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "# Parallel execution\n",
    "Jax tries to schedule calculations in parallel if it can. This may strongly increase memory usage. This does not hurt performance (and can slightly benefit performance, depending on the size of the dispatches.) In the case below, we can force sequential execution \n",
    "* with a for loop\n",
    "* With `optimization_barrier`\n",
    "\n",
    "Quoting [the jax documentation](https://docs.jax.dev/en/latest/_autosummary/jax.lax.optimization_barrier.html):\n",
    "\"An optimization barrier ensures that every output of the barrier that is used by any operator, has been evaluated before any operator that depends on one of the barrier’s outputs. This can be used to enforce a particular order of operations.\"\n",
    "\n",
    "So we can ensure sequential execution by making the inputs of the other operators depend on something in the optimization barrier -- e.g. x!\n",
    "\n",
    "> Note: in principle, jax \"rematerialization\" is intended to solve memory problems and rearange computations with barriers, just as we did here. In practice I have never found it to work properly though... E.g. in the example below we can get away with 2x temporaries, but rematerialization never achieves this... For now we should never rely on rematerialization to fix our memory problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_fft_multi(x, n=5):\n",
    "    res = 0\n",
    "    for i in range(n):\n",
    "        res = res + jnp.fft.irfftn((i+2)*jnp.fft.rfftn(x+i, axes=(1,2,3)), axes=(1,2,3))**(i+2)\n",
    "    return res\n",
    "f_fft_multi.jit = jax.jit(f_fft_multi, static_argnames=\"n\")\n",
    "show_hlo_info(f_fft_multi.jit, x3d, width=800, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_fft_multi_for(x, n=5):\n",
    "    def step(i, res):\n",
    "        return res + jnp.fft.irfftn((i+2)*jnp.fft.rfftn(x+i+1, axes=(1,2,3)), axes=(1,2,3))**(i+2)\n",
    "    return jax.lax.fori_loop(0, n, step, jnp.zeros_like(x))\n",
    "f_fft_multi_for.jit = jax.jit(f_fft_multi_for, static_argnames=\"n\")\n",
    "\n",
    "def f_fft_multi_barrier(x, n=5):\n",
    "    res = 0\n",
    "    for i in range(n):\n",
    "        res = res + jnp.fft.irfftn((i+2)*jnp.fft.rfftn(x+i, axes=(1,2,3)), axes=(1,2,3))**(i+2)\n",
    "        res,x = jax.lax.optimization_barrier((res, x))\n",
    "    return res\n",
    "f_fft_multi_barrier.jit = jax.jit(f_fft_multi_barrier, static_argnames=\"n\")\n",
    "\n",
    "x3d = jnp.zeros((2,64,64,32)) # Put a 2000 here, if you want to see \"rematerialization\" fail\n",
    "ns = np.arange(1, 10, dtype=int)\n",
    "nbytes = np.array([f_fft_multi.jit.lower(x3d, n=n).compile().memory_analysis().temp_size_in_bytes for n in ns])\n",
    "nbytes_for = np.array([f_fft_multi_for.jit.lower(x3d, n=n).compile().memory_analysis().temp_size_in_bytes for n in ns])\n",
    "nbytes_opt = np.array([f_fft_multi_barrier.jit.lower(x3d, n=n).compile().memory_analysis().temp_size_in_bytes for n in ns])\n",
    "\n",
    "plt.xlabel(\"n (number of parallel executable FFTs)\")\n",
    "plt.ylabel(\"Temporary memory usage [MB]\")\n",
    "plt.plot(ns, nbytes/1e6, marker=\"o\", label=\"python loop\")\n",
    "plt.plot(ns, nbytes_for/1e6, marker=\"o\", label=\"jax for loop\")\n",
    "plt.plot(ns, nbytes_opt/1e6, marker=\"x\", label=\"barriers\", ls=\"dashed\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "x3d = jnp.zeros((2,16,16,16)).block_until_ready()\n",
    "# Somehow the first run always takes longer, let's do it twice to fix the measurement\n",
    "print(\"Dummy:\")\n",
    "%timeit -n 100 f_fft_multi.jit(x3d, n=5).block_until_ready()\n",
    "%timeit -n 100 f_fft_multi_for.jit(x3d, n=5).block_until_ready()\n",
    "%timeit -n 100 f_fft_multi_barrier.jit(x3d, n=5).block_until_ready()\n",
    "print(\"Measurements:\")\n",
    "%timeit -n 100 f_fft_multi.jit(x3d, n=5).block_until_ready()\n",
    "%timeit -n 100 f_fft_multi_for.jit(x3d, n=5).block_until_ready()\n",
    "%timeit -n 100 f_fft_multi_barrier.jit(x3d, n=5).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "x3d = jnp.zeros((20,64,64,32))\n",
    "print(\"Dummy:\")\n",
    "%timeit -n 100 f_fft_multi.jit(x3d, n=5).block_until_ready()\n",
    "%timeit -n 100 f_fft_multi_for.jit(x3d, n=5).block_until_ready()\n",
    "%timeit -n 100 f_fft_multi_barrier.jit(x3d, n=5).block_until_ready()\n",
    "print(\"Measurements:\")\n",
    "%timeit -n 100 f_fft_multi.jit(x3d, n=5).block_until_ready()\n",
    "%timeit -n 100 f_fft_multi_for.jit(x3d, n=5).block_until_ready()\n",
    "%timeit -n 100 f_fft_multi_barrier.jit(x3d, n=5).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "x3d = jnp.zeros((200,64,64,32))\n",
    "print(\"Dummy:\")\n",
    "%timeit -n 100 f_fft_multi.jit(x3d, n=5).block_until_ready()\n",
    "%timeit -n 100 f_fft_multi_for.jit(x3d, n=5).block_until_ready()\n",
    "%timeit -n 100 f_fft_multi_barrier.jit(x3d, n=5).block_until_ready()\n",
    "print(\"Measurements:\")\n",
    "%timeit -n 100 f_fft_multi.jit(x3d, n=5).block_until_ready()\n",
    "%timeit -n 100 f_fft_multi_for.jit(x3d, n=5).block_until_ready()\n",
    "%timeit -n 100 f_fft_multi_barrier.jit(x3d, n=5).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "# End\n",
    "\n",
    "# Constant folding bug:\n",
    "Here is a bug I found in my experiments, I wrote a bug report on jax's github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If I understand this: https://docs.jax.dev/en/latest/internals/constants.html?utm_source=chatgpt.com\n",
    "# correctly, than I'd think that \"potential\" constants created through jax.numpy should never be folded\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "def f_arange_fine(x):\n",
    "    return x + jnp.arange(0, x.size).reshape(x.shape)\n",
    "\n",
    "def f_arange_bug(x):\n",
    "    return x + jnp.arange(1, x.size+1).reshape(x.shape)\n",
    "\n",
    "x = jnp.zeros((1024*32,2))\n",
    "print(jax.make_jaxpr(f_arange_fine)(x))\n",
    "print(jax.make_jaxpr(f_arange_bug)(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "# Intermediate recalculation question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_recalc(x):\n",
    "    get_im = lambda x: jnp.exp(jnp.arange(x.size).reshape(x.shape) ** 2.3)\n",
    "\n",
    "    # I use a jnp.sum here as a fusion barrier to create two different kernels\n",
    "    # Imagine this represents a long computation with many barriers\n",
    "    a = x + jnp.sum(x * get_im(x), axis=-1, keepdims=True) \n",
    "\n",
    "    # We reuse the same expression here, does it get recalculated or will jax create a temporary intermediate array?\n",
    "    # (In this function it recalculates it)\n",
    "    return a + get_im(x) \n",
    "\n",
    "def f_stored(x):\n",
    "    get_im = lambda x: jnp.sin(jnp.arange(x.size).reshape(x.shape) ** 2.3)\n",
    "\n",
    "    a = x + jnp.sum(get_im(x), axis=-1, keepdims=True) \n",
    " \n",
    "    # here get_im(x) gets saved and reused as a temporary device array\n",
    "    # (thanks to the subtle difference of using sin instead of exp)\n",
    "    return a + get_im(x) \n",
    "\n",
    "x = jnp.zeros((1024*128,2))\n",
    "comp1 = jax.jit(f_recalc).lower(x).compile()\n",
    "comp2 = jax.jit(f_stored).lower(x).compile()\n",
    "\n",
    "def create_svg(f_lowered, filename):\n",
    "    from jaxlib import xla_client as xc\n",
    "    from graphviz import Source\n",
    "    from IPython.display import SVG\n",
    "\n",
    "    mod = xc._xla.hlo_module_from_text(f_lowered.as_text())\n",
    "    dot = xc._xla.hlo_module_to_dot_graph(mod)\n",
    "    svg_text = Source(dot).pipe(format=\"svg\").decode(\"utf-8\")\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(svg_text)\n",
    "\n",
    "# create_svg(comp1, filename=\"f_recalc.svg\")\n",
    "# create_svg(comp2, filename=\"f_stored.svg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uvjax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
